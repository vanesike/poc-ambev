{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chamar o LLM #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "            azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            openai_api_key = os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "            deployment_name = os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "            api_version = \"2023-09-01-preview\",\n",
    "            temperature = 0.0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    You are an expert at selecting suppliers that will provide equipments to the company you work for.\n",
    "    The suppliers fill a spreadsheet with their machines' specifications and based on that, you check if the answers correspond to what your company requires to make them an official supplier.\n",
    "\n",
    "    In this task, you will analyze this data:\n",
    "    \n",
    "    ```\n",
    "    {df_data}\n",
    "    ```\n",
    "\n",
    "    For each supplier answer, you will check if the answer can fill the requirements according to the other fields and return a JSON.\n",
    "\n",
    "    GUIDELINES:\n",
    "    - In the dataframe, you must add another field called \"COMMENT\" and it should only contain \"OK\" or \"NOK\". \"OK\" in case the supplier's answer can fill the requirement or \"NOK\" in case the supplier's answer does not fill the requirement.\n",
    "    - You must return a valid JSON structure in your response, without any additional commentary, only the JSON.\n",
    "    - The JSON structure will be converted to a Dataframe, so return a structure that will make the conversion possible.\n",
    "    - If there's not enough information to make the analysis, in the field \"COMMENT\" just write \"Not enough information\".\n",
    "    - Don't evaluate the suppliers answers if you don't know if they fill the requirements.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"df_data\"]\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(df, supplier, section_name):\n",
    "    result = chain.invoke({\"df_data\": df})\n",
    "    pd.DataFrame(result).to_csv(f'result/filler/{section_name.replace(\"/\", \" \")}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrair e tratar dados da planilha de respostas do fornecedor #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(file_path, sheet_name, column_letters):\n",
    "    \"\"\"\n",
    "    Extracts data from specific columns in an Excel sheet.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input Excel file.\n",
    "        sheet_name (str): Name of the sheet to extract data from.\n",
    "        column_letters (list of str): List of column letters to extract data from.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the extracted data with column names from row 15.\n",
    "    \"\"\"\n",
    "    # Load workbook and sheet\n",
    "    wb = load_workbook(file_path, data_only=True)\n",
    "    sheet = wb[sheet_name]\n",
    "\n",
    "    # Read column names from row 15\n",
    "    column_names = [sheet[f'{col}15'].value for col in column_letters]\n",
    "\n",
    "    # Initialize a dictionary to store the data\n",
    "    data = {col: [] for col in column_names}\n",
    "\n",
    "    # Extract values from each column, ignoring the first 13 cells and row 15\n",
    "    for col_letter, column_name in zip(column_letters, column_names):\n",
    "        column_data = [cell.value for cell in sheet[col_letter] if cell.row > 13]\n",
    "        # Add data to the corresponding list\n",
    "        data[column_name] = column_data\n",
    "\n",
    "    # Find the maximum length of the lists\n",
    "    max_length = max(len(lst) for lst in data.values())\n",
    "\n",
    "    # Standardize the length of the lists with missing values (None)\n",
    "    for key in data:\n",
    "        while len(data[key]) < max_length:\n",
    "            data[key].append(None)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planilhas a serem extraídas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_khs = '../files/KHS.xlsx'\n",
    "file_path_krones = '../files/KRONES.xlsx'\n",
    "file_path_sidel = '../files/SIDEL.xlsx'\n",
    "sheet_filler = 'CAN Filler'\n",
    "column_filler = ['B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seções Can Filler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_filler_sections = {\n",
    "    \"1.0 PERFORMANCE and WARRANTY\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"2.0 GENERAL INFORMATION\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"3.0 PROCESS\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"4.0 CONSTRUCTIVE CHARACTERISTICS\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"5.0 FILLER ACCESSORIES\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"6.0 DIMENSIONS / WEIGHT\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"7.0 OXYGEN ELIMINATORS DEVICES\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"8.0 MATERIALS\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"9.0 CHANGE OVER\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"10.0 SAFETY\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"11.0 ELECTRICITY, CONTROL, ALARM, AUTOMATION\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"12.0 POWER CABINETS\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"13.0 UTILITIES CONSUMPTION\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"14.0 MAINTENANCE / DOCUMENTATION / TRAINING\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"15.0 TEC CAN FILLER ACCEPTANCE\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"16.0 ZONE DEMANDS DUE TO LOCAL REGULATIONS\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    },\n",
    "    \"17.0 HISTORY OF REVISIONS\": {\n",
    "        \"start\": \"\",\n",
    "        \"end\": \"\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificar os índices das seções:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_section(item):\n",
    "    return isinstance(item, str) and item in can_filler_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_index(df):\n",
    "    section_indexes = df[df['ITEM'].apply(is_section)].index.tolist()\n",
    "    section_indexes.append(len(df))  # adiciona o índice do final do DataFrame\n",
    "    return section_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_indexes(df, indexes, sections_dict):\n",
    "    for i in range(len(indexes) - 1):\n",
    "        start_idx = indexes[i]\n",
    "        end_idx = indexes[i + 1]\n",
    "        section_name = df.loc[start_idx, 'ITEM']\n",
    "        sections_dict[section_name][\"start\"] = start_idx + 1 \n",
    "        sections_dict[section_name][\"end\"] = end_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar planilhas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_df(df_supplier, sections, section_name):\n",
    "    df = df_supplier.iloc[sections[section_name]['start']:sections[section_name]['end']].reset_index(drop=True).rename(columns={'CNV': 'Charachteristic', 'UNIT': 'Instruction/Comments'}).drop(index=0, columns=['ITEM', 'ITEM DESCRIPTION', 'INDEX', 'Instruction / Comments', None]).to_csv(index=False)\n",
    "    call_llm(df, 'khs', section_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_df_performance(df_supplier, sections, section_name):\n",
    "    df = df_supplier.iloc[sections[section_name]['start']:sections[section_name]['end']].reset_index(drop=True).drop(index=[0], columns=['ITEM', None]).to_csv(index=False)\n",
    "    call_llm(df, 'khs', section_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_df_utilities(df_supplier, sections, section_name):\n",
    "    df = df_supplier.iloc[sections[section_name]['start']:sections[section_name]['end']].reset_index(drop=True).drop(index=[0], columns=['ITEM', 'ITEM DESCRIPTION', 'Instruction / Comments']).rename(columns={'CNV': 'Characteristic', 'INDEX': 'Instruction / Comments'}).dropna(subset=['Characteristic']).to_csv(index=False)\n",
    "    call_llm(df, 'khs', section_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_df_maintenance(df_supplier, sections, section_name):\n",
    "    df = df_supplier.iloc[sections[section_name]['start']:sections[section_name]['end']].reset_index(drop=True).drop(index=[0], columns=['ITEM', 'INDEX', 'ITEM DESCRIPTION', 'Instruction / Comments']).rename(columns={'CNV': 'Characteristic', 'UNIT': 'Instruction', None: 'Comments'}).dropna(subset=['Characteristic']).to_csv(index=False)\n",
    "    call_llm(df, 'khs', section_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_df_zone(df_supplier, sections, section_name):\n",
    "    df = df_supplier.iloc[sections[section_name]['start']:sections[section_name]['end']].reset_index(drop=True).drop(index=[0], columns=['ITEM', 'UNIT', 'INDEX']).rename(columns={'CNV': 'Zone', 'ITEM DESCRIPTION': 'Detail','Instruction / Comments': 'Instruction', None: 'Comments'}).to_csv(index=False)\n",
    "    call_llm(df, 'khs', section_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KHS: # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_khs = extract_info(file_path=file_path_khs, sheet_name=sheet_filler, column_letters=column_filler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Índices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khs_indexes = get_section_index(df_khs)\n",
    "_ = add_indexes(df_khs, khs_indexes, can_filler_sections)\n",
    "can_filler_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planilhas separadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "df_performance = separate_df_performance(df_khs, can_filler_sections, \"1.0 PERFORMANCE and WARRANTY\")\n",
    "df_general_info = separate_df(df_khs, can_filler_sections, \"2.0 GENERAL INFORMATION\")\n",
    "df_process = separate_df(df_khs, can_filler_sections, \"3.0 PROCESS\")\n",
    "\n",
    "time.sleep(15)\n",
    "\n",
    "df_characteristics = separate_df(df_khs, can_filler_sections, \"4.0 CONSTRUCTIVE CHARACTERISTICS\")\n",
    "df_accessories = separate_df(df_khs, can_filler_sections, \"5.0 FILLER ACCESSORIES\")\n",
    "df_dimensions = separate_df(df_khs, can_filler_sections, \"6.0 DIMENSIONS / WEIGHT\")\n",
    "\n",
    "time.sleep(15)\n",
    "\n",
    "df_oxygen = separate_df(df_khs, can_filler_sections, \"7.0 OXYGEN ELIMINATORS DEVICES\")\n",
    "df_materials = separate_df(df_khs, can_filler_sections, \"8.0 MATERIALS\")\n",
    "df_change = separate_df(df_khs, can_filler_sections, \"9.0 CHANGE OVER\")\n",
    "\n",
    "time.sleep(15)\n",
    "\n",
    "df_safety = separate_df(df_khs, can_filler_sections, \"10.0 SAFETY\")\n",
    "df_electricity = separate_df(df_khs, can_filler_sections, \"11.0 ELECTRICITY, CONTROL, ALARM, AUTOMATION\")\n",
    "df_power = separate_df(df_khs, can_filler_sections, \"12.0 POWER CABINETS\")\n",
    "\n",
    "time.sleep(15)\n",
    "\n",
    "df_utilities = separate_df_utilities(df_khs, can_filler_sections, \"13.0 UTILITIES CONSUMPTION\")\n",
    "df_maintenance = separate_df_maintenance(df_khs, can_filler_sections, \"14.0 MAINTENANCE / DOCUMENTATION / TRAINING\")\n",
    "df_acceptance = separate_df(df_khs, can_filler_sections, \"15.0 TEC CAN FILLER ACCEPTANCE\")\n",
    "df_zone = separate_df_zone(df_khs, can_filler_sections, \"16.0 ZONE DEMANDS DUE TO LOCAL REGULATIONS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo de score #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = 'result/filler'\n",
    "comment_column = []\n",
    "\n",
    "for filename in os.listdir(files):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(files, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        col = df.iloc[:, -1]\n",
    "        comment_column.append(col)\n",
    "\n",
    "comments = pd.concat(comment_column, axis=0).reset_index(drop=True)\n",
    "df_comments = pd.DataFrame(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_column = df_comments['COMMENT']\n",
    "\n",
    "total_comments = len(df_comments)\n",
    "total_ok = df_comments[comment_column == 'OK'].count()\n",
    "total_nok = df_comments[comment_column == 'NOK'].count()\n",
    "total_not_enough = df_comments[comment_column == 'Not enough information'].count()\n",
    "\n",
    "percentage_ok = float((total_ok / total_comments) * 100)\n",
    "percentage_nok = float((total_nok / total_comments) * 100)\n",
    "percentage_not_enough = float((total_not_enough / total_comments) * 100)\n",
    "\n",
    "scores = {\n",
    "    \"OK\": f'{percentage_ok:.2f}%',\n",
    "    \"NOK\": f'{percentage_nok:.2f}%',\n",
    "    \"NOT ENOUGH INFO\": f'{percentage_not_enough:.2f}%'\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(list(scores.items()), columns=['Category', 'Percentage'])\n",
    "df.to_csv('result/filler/scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquivo final #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = 'result/filler'\n",
    "excel_writer = pd.ExcelWriter('result/filler/can_filler.xlsx', engine='openpyxl')\n",
    "\n",
    "for filename in os.listdir(files):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(files, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # add df to excel file as a tab\n",
    "        sheet_name = os.path.splitext(filename)[0]\n",
    "        df.to_excel(excel_writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "excel_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
