{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../files/KHS.xlsx\", sheet_name=\"CAN Filler\", header=None)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance and Warranty #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_start = df[df[1] == '1.0 PERFORMANCE and WARRANTY'].index[0]\n",
    "general_info_start = df[df[1] == '2.0 GENERAL INFORMATION'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = df.loc[performance_start + 1 : general_info_start - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df.columns = performance_df.iloc[0]\n",
    "performance_df = performance_df[1:]\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = performance_df[[\"CNV\", \"ITEM DESCRIPTION\", \"UNIT\", \"INDEX\", \"Instruction / Comments\", \"Supplier Answers\"]]\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df_json = performance_df.to_json(orient='records', index=False)\n",
    "json_str = json.loads(df_json)\n",
    "json_performance = json.dumps(json_str, indent=2, sort_keys=False, ensure_ascii=False)\n",
    "print(json_performance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructive characteristics #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructive_start = df[df[1] == '4.0 CONSTRUCTIVE CHARACTERISTICS'].index[0]\n",
    "accessories_info_start = df[df[1] == '5.0 FILLER ACCESSORIES'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructive_df = df.loc[constructive_start + 1 : accessories_info_start - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructive_df = constructive_df.rename(columns={1: \"Item\", 2: \"Characteristic\", 4: \"Unit\", 5: \"Instruction / Comments\", 8: \"Supplier Answers\"}).iloc[1: , :].drop(columns=[0, 3, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df_json = constructive_df.to_json(orient='records', index=False)\n",
    "json_str = json.loads(df_json)\n",
    "json_performance = json.dumps(json_str, indent=2, sort_keys=False, ensure_ascii=False)\n",
    "print(json_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Classification #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser\n",
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "            azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            openai_api_key = os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "            deployment_name = os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "            api_version = \"2023-09-01-preview\",\n",
    "            temperature = 0.0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceEvaluation(BaseModel):\n",
    "    CNV: str = Field(default=None),\n",
    "    INDEX: str = Field(default=None),\n",
    "    DESCRIPTION: str = Field(default=None),\n",
    "    Instruction: str = Field(default=None),\n",
    "    SUPPLIER_ANSWERS: str = Field(default=None),\n",
    "    UNIT: str = Field(default=None),\n",
    "    COMMENT: str = Field(default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputParser()\n",
    "# parser = JsonOutputParser(pydantic_object=PerformanceEvaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = PydanticOutputParser(pydantic_object=PerformanceEvaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    You are an expert at selecting suppliers that will provide equipments to the company you work for.\n",
    "    The suppliers fill a spreadsheet with their machines' specifications and based on that, you check if the answers correspond to what your company requires to make them an official supplier.\n",
    "\n",
    "    In this task, you will analyze this data:\n",
    "    \n",
    "    ```\n",
    "    {json_data}\n",
    "    ```\n",
    "\n",
    "    For each JSON object, you will check if the supplier's answer can fill the requirements according to the other fields.\n",
    "    The CNV field describes what is being analyzed, in case you need more information.\n",
    "\n",
    "    GUIDELINES:\n",
    "    - For each JSON object, you should add another field called \"COMMENT\" and it should only contain \"OK\" or \"NOK\". \"OK\" in case the supplier's answer can fill the requirement or \"NOK\" in case the supplier's answer does not fill the requirement.\n",
    "    - For each JSON object, you should add another field called \"REASON\", and it should contain the reason for you to label the answer as \"OK\" or \"NOK\".\n",
    "    - Your response should only contain a valid JSON with the analysis made.\n",
    "    - If there's not enough information to make the analysis, in the field \"COMMENT\" just write \"Not enough information\".\n",
    "    - Don't evaluate the suppliers answers if you don't know if they fill the requirements.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    You are an expert at selecting suppliers that will provide equipments to the company you work for.\n",
    "    The suppliers fill a spreadsheet with their machines' specifications and based on that, you check if the answers correspond to what your company requires to make them an official supplier.\n",
    "\n",
    "    In this task, you will analyze this data:\n",
    "    \n",
    "    ```\n",
    "    {json_data}\n",
    "    ```\n",
    "\n",
    "    For each JSON object, you will check if the supplier's answer can fill the requirements according to the ITEM DESCRIPTION and/or the INDEX fields.\n",
    "    The CNV field describes what is being analyzed, in case you need more information.\n",
    "\n",
    "    GUIDELINES:\n",
    "    - For each JSON object, you should add another field called \"COMMENT\" and it should only contain \"OK\" or \"NOK\". \"OK\" in case the supplier's answer can fill the requirement or \"NOK\" in case the supplier's answer does not fill the requirement.\n",
    "    - For each JSON object, you should add another field called \"REASON\", and it should contain the reason for you to label the answer as \"OK\" or \"NOK\".\n",
    "    - Your response should only contain a valid JSON with the analysis made.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"json_data\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"json_data\": json_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
